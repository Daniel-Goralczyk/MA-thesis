{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating RoBERTa scores \n",
    "Load the necessary packages and load the pickle file with the gathered sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Specify the path to the saved pickle file\n",
    "load_path = r'video_sentiment_scores_local_gpu.pkl'\n",
    "\n",
    "# Load the pickle file\n",
    "with open(load_path, 'rb') as file:\n",
    "    loaded_video_sentiment_scores = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose three videos to print to see what the output was. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: Xd9Aj9PYats, Type: <class 'list'>\n",
      "Value: [(array([[-2.8199172 ,  0.23089172,  2.9052267 ]], dtype=float32), 180)]\n",
      "\n",
      "Key: TGiyIQ2SC9U, Type: <class 'list'>\n",
      "Value: [(array([[ 0.7475943,  0.5757689, -1.5683148]], dtype=float32), 510), (array([[ 0.40991086,  0.77049816, -1.4032675 ]], dtype=float32), 510), (array([[ 0.6675381 ,  0.64852756, -1.5578878 ]], dtype=float32), 510), (array([[ 0.7406259,  0.6555336, -1.598153 ]], dtype=float32), 93)]\n",
      "\n",
      "Key: a_DKejYUb6E, Type: <class 'list'>\n",
      "Value: [(array([[-2.425388 ,  1.211941 ,  1.1355975]], dtype=float32), 510), (array([[-0.8975626,  1.7862841, -1.2017012]], dtype=float32), 47)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define keys you want to print\n",
    "keys_to_print = ['Xd9Aj9PYats', 'TGiyIQ2SC9U', 'a_DKejYUb6E']\n",
    "\n",
    "# Print only specific keys from the loaded data\n",
    "for key in keys_to_print:\n",
    "    if key in loaded_video_sentiment_scores:\n",
    "        value = loaded_video_sentiment_scores[key]\n",
    "        print(f\"Key: {key}, Type: {type(value)}\")\n",
    "        print(f\"Value: {value}\\n\")\n",
    "    else:\n",
    "        print(f\"Key '{key}' not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create an empty list. Then weigh each chunk before taking the average for each video. Then append the list with the new values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_elements_list = []\n",
    "\n",
    "# Assuming loaded_data_cpu is a dictionary with video IDs as keys and lists of tuples as values\n",
    "for video_id, scores in loaded_video_sentiment_scores.items():\n",
    "    if isinstance(scores, list) and len(scores) > 0:\n",
    "        total_positive, total_neutral, total_negative = 0, 0, 0\n",
    "        total_tokens = sum(weight for _, weight in scores)  # Summing up the weights for total_tokens\n",
    "        chunk_count = len(scores)  # Count of chunks for each video ID\n",
    "        \n",
    "        for tensor_scores, weight in scores:\n",
    "            if isinstance(tensor_scores, np.ndarray) and tensor_scores.size > 0:\n",
    "                if tensor_scores.shape[1] >= 3:  # Check the number of columns in the array\n",
    "                    total_negative += tensor_scores[0, 0] * weight\n",
    "                    total_neutral += tensor_scores[0, 1] * weight\n",
    "                    total_positive += tensor_scores[0, 2] * weight\n",
    "\n",
    "        if chunk_count > 0:\n",
    "            positive_avg = total_positive / total_tokens\n",
    "            neutral_avg = total_neutral / total_tokens\n",
    "            negative_avg = total_negative / total_tokens\n",
    "            \n",
    "            positive_avg = format(positive_avg, '.8f')\n",
    "            neutral_avg = format(neutral_avg, '.8f')\n",
    "            negative_avg = format(negative_avg, '.8f')\n",
    "            \n",
    "            # Append the averages with labels to the list for each video ID\n",
    "            aggregated_elements_list.append((video_id, {'neg': negative_avg, 'neu': neutral_avg, 'pos': positive_avg, 'total_tokens': total_tokens, 'chunk_count': chunk_count}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick print of the end result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Xd9Aj9PYats', {'neg': '-2.81991720', 'neu': '0.23089172', 'pos': '2.90522671', 'total_tokens': 180, 'chunk_count': 1})\n",
      "('mj3KdTI-MJc', {'neg': '-2.17857313', 'neu': '0.63513565', 'pos': '1.47983086', 'total_tokens': 278, 'chunk_count': 1})\n",
      "('NB1MbFGLP-4', {'neg': '-3.39951777', 'neu': '0.45537752', 'pos': '3.10518146', 'total_tokens': 111, 'chunk_count': 1})\n",
      "('BNzOf0LMMmU', {'neg': '-3.14023209', 'neu': '0.71023858', 'pos': '2.36409974', 'total_tokens': 79, 'chunk_count': 1})\n",
      "('jmSIiXWTuXU', {'neg': '-2.24710751', 'neu': '0.80607367', 'pos': '1.01895618', 'total_tokens': 78, 'chunk_count': 1})\n",
      "('hptX6cW2ECo', {'neg': '-2.48175192', 'neu': '1.62193692', 'pos': '0.59548968', 'total_tokens': 119, 'chunk_count': 1})\n",
      "('GRs2eMGVQzk', {'neg': '-2.43378901', 'neu': '0.42441916', 'pos': '2.10528517', 'total_tokens': 178, 'chunk_count': 1})\n",
      "('G3ZfdXs0-9I', {'neg': '-1.24455845', 'neu': '0.80801034', 'pos': '0.13362151', 'total_tokens': 93, 'chunk_count': 1})\n",
      "('Qsp9UZE4meE', {'neg': '-0.41269368', 'neu': '0.92122293', 'pos': '-0.63220417', 'total_tokens': 95, 'chunk_count': 1})\n",
      "('HTsdJOz_xPk', {'neg': '0.76790667', 'neu': '0.36926359', 'pos': '-1.38071835', 'total_tokens': 185, 'chunk_count': 1})\n",
      "('ILxNd2QIasc', {'neg': '0.86079097', 'neu': '0.59482419', 'pos': '-1.81710672', 'total_tokens': 76, 'chunk_count': 1})\n",
      "('x2ZNXlO-aLo', {'neg': '0.82527608', 'neu': '0.44449520', 'pos': '-1.61904311', 'total_tokens': 165, 'chunk_count': 1})\n",
      "('tXbrcJT9zGs', {'neg': '-1.56507695', 'neu': '1.26846218', 'pos': '0.17857516', 'total_tokens': 145, 'chunk_count': 1})\n",
      "('jz18zZa2z6U', {'neg': '-1.16076016', 'neu': '0.54917455', 'pos': '0.48032698', 'total_tokens': 155, 'chunk_count': 1})\n",
      "('GF7dN0SA_qo', {'neg': '0.13688311', 'neu': '0.59812629', 'pos': '-1.05859411', 'total_tokens': 219, 'chunk_count': 1})\n",
      "('9N5TxhlZpvY', {'neg': '-3.52765059', 'neu': '1.32053888', 'pos': '2.09177899', 'total_tokens': 60, 'chunk_count': 1})\n",
      "('mk2CpplfyjQ', {'neg': '-3.08018351', 'neu': '0.48884237', 'pos': '2.75498867', 'total_tokens': 123, 'chunk_count': 1})\n",
      "('-s5EvgVDmh4', {'neg': '-3.27245164', 'neu': '0.55138302', 'pos': '2.80509734', 'total_tokens': 143, 'chunk_count': 1})\n",
      "('sokOOEAxWlc', {'neg': '-2.46563697', 'neu': '1.03979933', 'pos': '1.37823927', 'total_tokens': 374, 'chunk_count': 1})\n",
      "('-74AZF9v5yY', {'neg': '0.20239741', 'neu': '0.83481997', 'pos': '-1.13330209', 'total_tokens': 76, 'chunk_count': 1})\n"
     ]
    }
   ],
   "source": [
    "for item in aggregated_elements_list[:20]:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then save the output into a CSV to be used in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'aggregated_elements_list.csv'\n",
    "with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    \n",
    "    # Write header\n",
    "    csv_writer.writerow(['video_id', 'neg', 'neu', 'pos', 'total_tokens', 'chunk_count'])\n",
    "    \n",
    "    # Write data\n",
    "    for video_id, aggregated_data in aggregated_elements_list:\n",
    "        csv_writer.writerow([video_id, aggregated_data['neg'], aggregated_data['neu'], \n",
    "                             aggregated_data['pos'], aggregated_data['total_tokens'], \n",
    "                             aggregated_data['chunk_count']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
